{
  "query": "How do I use Python for web scraping?",
  "results": [
    {
      "content": "from typing import Dict, Optional, Literal, List, Any\nfrom mcp.server.fastmcp import FastMCP, tool\nimport httpx\nfrom bs4 import BeautifulSoup\nimport trafilatura\nmcp = FastMCP(\"url-reader\")\ndef *fetch(url: str, timeout: float, max*bytes: int, headers: Optional[Dict[str, str]]) -> tuple[bytes, Dict[str, Any]]:\n    default_headers = {\n        \"user-agent\": \"url-reader-mcp/1.0 (+https://modelcontextprotocol.io)\",\n        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    }\n    if headers:\n        default_headers.update(headers)\n    with httpx.Client(follow_redirects=True, timeout=timeout, headers=default_headers) as client:\n        r = client.get(url)\n        r.raise_for_status()\n        content = r.content if len(r.content) <= max_bytes else r.content[:max_bytes]\n        meta = {\n            \"status\": r.status_code,\n            \"final_url\": str(r.url),\n            \"content_type\": r.headers.get(\"content-type\", \"\"),\n        }\n        return content, meta\ndef *meta*from_html(html: str, url: str) -> Dict[str, str]:\n    soup = BeautifulSoup(html, \"lxml\")\n    def m(name, prop=None):\n        if name:\n            tag = soup.find(\"meta\", attrs={\"name\": name})\n            if tag and tag.get(\"content\"):\n                return tag[\"content\"].strip()\n        if prop:\n            tag = soup.find(\"meta\", attrs={\"property\": prop})\n            if tag and tag.get(\"content\"):\n                return tag[\"content\"].strip()\n        return \"\"\n    title = (m(None, \"og:title\") or (soup.title.string.strip() if soup.title and soup.title.string else \"\"))\n    description = (m(\"description\") or m(None, \"og:description\"))\n    return {\"url\": url, \"title\": title, \"description\": description}\n@tool\ndef read_url(\n    url: str,\n    mode: Literal[\"article\", \"raw\", \"selectors\"] = \"article\",\n    selectors: Optional[Dict[str, str]] = None,\n    include_html: bool = False,\n    timeout: float = 15.0,\n    max_bytes: int = 2_000_000,\n    headers: Optional[Dict[str, str]] = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fetch a URL and return:\n      - article: cleaned article text + metadata (via trafilatura)\n      - raw: meta + (optional) raw HTML\n      - selectors: extract fields via CSS selectors {name: selector}\n    \"\"\"\n    content, meta_http = *fetch(url, timeout, max*bytes, headers)\n    html = content.decode(\"utf-8\", errors=\"replace\")\n    base_meta = *meta*from_html(html, meta_http.get(\"final_url\") or url)\n    base_meta.update(meta_http)\n    if mode == \"raw\":\n        return {\"mode\": \"raw\", \"meta\": base_meta, \"html\": html if include_html else None}\n    if mode == \"selectors\":\n        soup = BeautifulSoup(html, \"lxml\")\n        out: Dict[str, Any] = {}\n        for key, sel in (selectors or {}).items():\n            nodes = soup.select(sel)\n            if not nodes:\n                out[key] = None\n            elif len(nodes) == 1:\n                out[key] = nodes[0].get_text(strip=True)\n            else:\n                out[key] = [n.get_text(strip=True) for n in nodes if n.get_text(strip=True)]\n        return {\"mode\": \"selectors\", \"meta\": base_meta, \"data\": out}\n    # mode == \"article\"\n    extracted = trafilatura.extract(\n        html,\n        output=\"json\",\n        with_metadata=True,\n        include_comments=False,\n        include_images=False,\n        url=base_meta[\"url\"],\n    )\n    return {\n        \"mode\": \"article\",\n        \"meta\": base_meta,\n        \"article\": extracted,  # JSON string from trafilatura\n        \"include_html\": include_html,\n        \"html\": html if include_html else None,\n    }\nif **name** == \"__main__\":\n    mcp.run()",
      "distance": 0.44028875587644734,
      "id": "b2025203-c403-407a-9dd5-5845958ef1b4",
      "metadata": {
        "conversation_id": "b2025203-c403-407a-9dd5-5845958ef1b4",
        "earliest_ts": "2025-09-25T23:18:26.521206-05:00",
        "id": "b2025203-c403-407a-9dd5-5845958ef1b4",
        "latest_ts": "2025-09-25T23:18:26.521206-05:00",
        "message_count": 1,
        "message_id": "db66a86e-16c2-4a2a-9c9a-c54f902525c4",
        "role": "user",
        "source": "postgres",
        "title": "Docker MCP gateway configuration"
      },
      "preview": "from typing import Dict, Optional, Literal, List, Any\nfrom mcp.server.fastmcp import FastMCP, tool\nimport httpx\nfrom bs4 import BeautifulSoup\nimport trafilatura\nmcp = FastMCP(\"url-reader\")\ndef *fetch(url: str, timeout: float, max*bytes: int, headers: Optional[Dict[str, str]]) -> tuple[bytes, Dict[str, Any]]:\n    default_headers = {\n        \"user-agent\": \"url-reader-mcp/1.0 (+https://modelcontextprotocol.io)\",\n        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n  ...",
      "relevance": 0.5597112441235527,
      "source": "postgres",
      "title": "Docker MCP gateway configuration"
    },
    {
      "content": "How do I run a python script",
      "distance": 0.44823754450823783,
      "id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
      "metadata": {
        "conversation_id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
        "earliest_ts": "2025-03-04T12:15:35.393452-06:00",
        "id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
        "latest_ts": "2025-03-04T12:15:35.393452-06:00",
        "message_count": 1,
        "message_id": "8cdc85ef-d30a-42b8-9434-fefef2f29243",
        "role": "user",
        "source": "postgres",
        "title": "Install Python on M3"
      },
      "preview": "How do I run a python script",
      "relevance": 0.5517624554917622,
      "source": "postgres",
      "title": "Install Python on M3"
    },
    {
      "content": " ```python\n# 1_crawl_all.py\nimport requests\nimport json\nfrom datetime import datetime\n# HARDCODED CONFIG\nTOKEN = \"H1qQ0oLO-NUfbWp41TLqsGs6rRW6vMMUQzim-cPtjV0\"\nCUSTOMER_ID = \"188\"\nLIBRARY = \"chicagoactive\"\nBASE_URL = \"https://cloudimanage.com/work/api/v2\"\ndef crawl_library():\n    \"\"\"Crawl entire library using pagination.\"\"\"\n    all_docs = []\n    cursor = None\n    page = 1\n    while True:\n        print(f\"\ud83d\udce5 Page {page}...\", end=\" \")\n        # Build request payload\n        payload = {\"limit\": 1000}\n        if cursor:\n            payload[\"cursor\"] = cursor\n        # Make request\n        response = requests.post(\n            f\"{BASE_URL}/customers/{CUSTOMER_ID}/libraries/{LIBRARY}/sync/documents/crawl\",\n            json=payload,\n            headers={\n                \"Content-Type\": \"application/json\",\n                \"X-Auth-Token\": TOKEN\n            }\n        )\n        # Check for errors\n        if response.status_code != 200:\n            print(f\"\\n\u274c Error: {response.status_code}\")\n            print(f\"Response: {response.text}\")\n            break\n        # Parse response\n        data = response.json()\n        docs = data.get('data', [])\n        all_docs.extend(docs)\n        print(f\"\u2705 Got {len(docs)} docs (Total: {len(all_docs)})\")\n        # Check for more pages\n        cursor = data.get('cursor')\n        if not cursor:\n            print(f\"\\n\ud83c\udfc1 Complete: {len(all_docs)} documents\")\n            break\n        page += 1\n    return all_docs\n# Main execution\nprint(\"\ud83d\ude80 Starting crawl of chicagoactive library...\\n\")\ntry:\n    documents = crawl_library()\n    # Save with timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"imanage_crawl_{timestamp}.json\"\n    with open(filename, 'w') as f:\n        json.dump(documents, f, indent=2)\n    print(f\"\\n\ud83d\udcbe Saved to {filename}\")\n    print(f\"\ud83d\udcc8 File size: {len(json.dumps(documents)) / 1024 / 1024:.2f} MB\")\n    print(f\"\ud83d\udcca Total documents: {len(documents)}\")\n    # Print sample doc structure\n    if documents:\n        print(\"\\n\ud83d\udccb Sample document structure:\")\n        print(json.dumps(documents[0], indent=2)[:500] + \"...\")\nexcept Exception as e:\n    print(f\"\\n\u274c Error: {e}\")\n    import traceback\n    traceback.print_exc()\n```\nCommon issues:\n- Token expired? (try regenerating)\n- Library name wrong? (case sensitive: \"chicagoactive\" vs \"ChicagoActive\")\n- Permission issue? (did the god-mode permission actually get applied?)\n- Rate limiting?",
      "distance": 0.48466042141304944,
      "id": "a00541ea-8870-4c64-9be6-130dce1d8cae",
      "metadata": {
        "conversation_id": "a00541ea-8870-4c64-9be6-130dce1d8cae",
        "earliest_ts": "2025-10-03T15:30:03.534888-05:00",
        "id": "a00541ea-8870-4c64-9be6-130dce1d8cae",
        "latest_ts": "2025-10-03T15:30:03.534888-05:00",
        "message_count": 1,
        "message_id": "1df11085-cc68-4d88-a209-62525248a5c5",
        "role": "assistant",
        "source": "postgres",
        "title": "Imanage MCP server integration"
      },
      "preview": " ```python\n# 1_crawl_all.py\nimport requests\nimport json\nfrom datetime import datetime\n# HARDCODED CONFIG\nTOKEN = \"H1qQ0oLO-NUfbWp41TLqsGs6rRW6vMMUQzim-cPtjV0\"\nCUSTOMER_ID = \"188\"\nLIBRARY = \"chicagoactive\"\nBASE_URL = \"https://cloudimanage.com/work/api/v2\"\ndef crawl_library():\n    \"\"\"Crawl entire library using pagination.\"\"\"\n    all_docs = []\n    cursor = None\n    page = 1\n    while True:\n        print(f\"\ud83d\udce5 Page {page}...\", end=\" \")\n        # Build request payload\n        payload = {\"limit\": 1000}\n...",
      "relevance": 0.5153395785869506,
      "source": "postgres",
      "title": "Imanage MCP server integration"
    },
    {
      "content": " That error is from OpenWebUI trying to load your Function. OpenWebUI expects a specific class structure.\nHere's the corrected version:\n```python\n\"\"\"\ntitle: DovOS Corpus Query\nauthor: Dov\nversion: 0.1.0\nrequired_open_webui_version: 0.3.9\n\"\"\"\nimport requests\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nclass Tools:\n    def __init__(self):\n        self.valves = self.Valves()\n    class Valves(BaseModel):\n        RAG_API_URL: str = Field(\n            default=\"http://localhost:9000\",\n            description=\"Base URL for your RAG API service\"\n        )\n        DEFAULT_RESULTS: int = Field(\n            default=3,\n            description=\"Number of results to retrieve\"\n        )\n    def query_corpus(\n        self, \n        query: str,\n        __user__: dict = {},\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ) -> str:\n        \"\"\"\n        Query your DovOS corpus for relevant past conversations and context.\n        Use this when you need to search through your conversation history.\n        :param query: The search query to find relevant content in your corpus\n        :return: Formatted results from your corpus\n        \"\"\"\n        try:\n            response = requests.post(\n                f\"{self.valves.RAG_API_URL}/rag/search\",\n                json={\n                    \"query\": query,\n                    \"n_results\": self.valves.DEFAULT_RESULTS,\n                    \"semantic_weight\": 0.7\n                },\n                timeout=10\n            )\n            if response.status_code == 200:\n                data = response.json()\n                results = data.get(\"results\", [])\n                if not results:\n                    return f\"No results found in corpus for: '{query}'\"\n                # Format results\n                formatted = [f\"Found {len(results)} results for '{query}':\\n\"]\n                for i, result in enumerate(results, 1):\n                    meta = result.get(\"metadata\", {})\n                    score = result.get(\"combined_score\", 0)\n                    preview = result.get(\"preview\", \"\")\n                    formatted.append(f\"\"\"\n- Source: {meta.get('source', 'Unknown')}\n- Date: {meta.get('create_time', 'Unknown')}\n{preview}\n---\n\"\"\")\n                return \"\\n\".join(formatted)\n            else:\n                return f\"Error querying corpus: HTTP {response.status_code}\"\n        except Exception as e:\n            return f\"Error querying corpus: {str(e)}\"\n```\n1. Class must be named `Tools` (not `Filter`)\n2. Each method becomes a callable tool\n3. Use `__user__` and `__event_emitter__` parameters for OpenWebUI integration\n4. Return a string that becomes the tool's output\n1. OpenWebUI \u2192 Workspace \u2192 Tools\n2. Click \"+\" \n3. Paste the code above\n4. Save\n5. Configure the RAG_API_URL in Valves if needed\nJust ask naturally and OpenWebUI will offer to call the tool:\n```\nUser: Search my corpus for conversations about attachment patterns\n[OpenWebUI will show: \"Use query_corpus tool?\"]\n```\nOr the LLM will automatically decide to call it when relevant.\n- **Tools**: Explicit, LLM decides when to search corpus\n- **Filter**: Automatic, injects context before LLM sees the message",
      "distance": 0.5263846390797429,
      "id": "48d60430-2d69-4083-a2fe-188ed6f613cc",
      "metadata": {
        "conversation_id": "48d60430-2d69-4083-a2fe-188ed6f613cc",
        "earliest_ts": "2025-10-05T22:45:38.211672-05:00",
        "id": "48d60430-2d69-4083-a2fe-188ed6f613cc",
        "latest_ts": "2025-10-05T22:45:38.211672-05:00",
        "message_count": 1,
        "message_id": "e67593e6-e08b-4d6f-8fab-f03d39f4ff08",
        "role": "assistant",
        "source": "postgres",
        "title": "RAG API service architecture"
      },
      "preview": " That error is from OpenWebUI trying to load your Function. OpenWebUI expects a specific class structure.\nHere's the corrected version:\n```python\n\"\"\"\ntitle: DovOS Corpus Query\nauthor: Dov\nversion: 0.1.0\nrequired_open_webui_version: 0.3.9\n\"\"\"\nimport requests\nfrom typing import Optional, Callable, Awaitable\nfrom pydantic import BaseModel, Field\nclass Tools:\n    def __init__(self):\n        self.valves = self.Valves()\n    class Valves(BaseModel):\n        RAG_API_URL: str = Field(\n            default...",
      "relevance": 0.47361536092025713,
      "source": "postgres",
      "title": "RAG API service architecture"
    },
    {
      "content": "No available formula with the name \"python-requests\". Did you mean python-yq?",
      "distance": 0.528946778492092,
      "id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
      "metadata": {
        "conversation_id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
        "earliest_ts": "2025-03-04T12:25:46.473868-06:00",
        "id": "097bf57c-d555-4adc-b90d-f9e6551eb25d",
        "latest_ts": "2025-03-04T12:25:46.473868-06:00",
        "message_count": 1,
        "message_id": "95d7cff2-9d12-4917-9998-9e89f63f2603",
        "role": "user",
        "source": "postgres",
        "title": "Install Python on M3"
      },
      "preview": "No available formula with the name \"python-requests\". Did you mean python-yq?",
      "relevance": 0.47105322150790796,
      "source": "postgres",
      "title": "Install Python on M3"
    }
  ],
  "search_type": "semantic"
}