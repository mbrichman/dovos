import json
import os
import typer
from docx import Document
from sentence_transformers import SentenceTransformer
import chromadb
import re
from datetime import datetime

app = typer.Typer()

# === CONFIG ===
COLLECTION_NAME = "chat_history"
PERSIST_DIR = "./chroma_storage"

# === INIT ===
print("Loading embedding model...")
embedder = SentenceTransformer("all-MiniLM-L6-v2")
chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)
collection = chroma_client.get_or_create_collection(COLLECTION_NAME)


@app.command()
def search(
    query: str = typer.Argument(..., help="Search query"),
    top_k: int = 5,
    keyword: bool = typer.Option(
        False,
        "--keyword",
        "-k",
        help="Use literal keyword search instead of semantic search",
    ),
):
    print("üîç Running search...")
    print("üìÑ Document count in DB:", collection.count())

    results = {"documents": [[]], "metadatas": [[]]}

    if keyword:
        print("üîé Using keyword-based Boolean search.")
        all_docs = collection.get(include=["documents", "metadatas"], limit=9999)
        terms = (
            query.replace("AND", "&&").replace("OR", "||").replace("NOT", "!!").split()
        )
        matches = []

        def match(doc):
            text = doc.lower()
            expr = ""
            for term in terms:
                if term == "&&":
                    expr += " and "
                elif term == "||":
                    expr += " or "
                elif term == "!!":
                    expr += " not "
                else:
                    expr += f"'{term.lower()}' in text"
            try:
                return eval(expr)
            except Exception:
                return False

        for idx, doc in enumerate(all_docs["documents"]):
            if match(doc):
                matches.append((doc, all_docs["metadatas"][idx]))

        if not matches:
            print("‚ùå No keyword matches found.")
            return

        print(f"‚úÖ Found {len(matches)} keyword matches.")
        for idx, (doc, meta) in enumerate(matches[:top_k]):
            print(f"\nResult {idx+1}: {meta.get('title', 'Untitled')}")
            print(doc[:300] + "...\n")
            results["documents"][0].append(doc)
            results["metadatas"][0].append(meta)

    else:
        embedding = embedder.encode([query])[0]
        results = collection.query(
            query_embeddings=[embedding.tolist()], n_results=top_k
        )

        if not results["documents"][0]:
            print("‚ùå No semantic matches found.")
            return

        for idx, (doc, meta) in enumerate(
            zip(results["documents"][0], results["metadatas"][0])
        ):
            print(f"\nResult {idx+1}: {meta.get('title', 'Untitled')}")
            print(doc[:300] + "...\n")

    try:
        choice = input(
            "\nEnter result number to view as HTML in Safari (or press Enter to skip): "
        ).strip()
    except (EOFError, KeyboardInterrupt):
        print("\n‚è≠Ô∏è Skipped.")
        return

    if not choice:
        return

    if choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(results["documents"][0]):
            full_text = results["documents"][0][idx]
            title = (
                results["metadatas"][0][idx]
                .get("title", "Untitled")
                .replace(" ", "_")[:30]
            )

            with open("debug_output.md", "w", encoding="utf-8") as f:
                f.write(full_text)

            import markdown
            import tempfile

            html_body = markdown.markdown(
                full_text, extensions=["sane_lists", "tables", "nl2br"]
            )
            html_full = f"""
            <html><head><meta charset="UTF-8"><title>{title}</title>
            <style>
            body {{ font-family: Georgia, serif; max-width: 700px; margin: auto; line-height: 1.6; }}
            </style></head><body>{html_body}</body></html>"""
            with tempfile.NamedTemporaryFile(
                "w", delete=False, suffix=".html", encoding="utf-8"
            ) as tmp:
                tmp.write(html_full)
                tmp_path = tmp.name
            os.system(f'open -a Safari "{tmp_path}"')
        else:
            print("‚ùå Invalid result number.")
    else:
        print("‚ùå Invalid input.")


@app.command()
def index(
    chat_json_path: str = typer.Argument(
        ..., help="Path to your conversations.json file"
    )
):
    with open(chat_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    conversations = data if isinstance(data, list) else data.get("conversations", [])
    documents, metadatas, ids = [], [], []

    for idx, conv in enumerate(conversations):
        messages = []
        message_map = conv.get("mapping", {})
        ordered = sorted(message_map.values(), key=lambda m: m.get("create_time", 0))

        for msg in ordered:
            message = msg.get("message")
            if not message:
                continue

            role = message.get("author", {}).get("role", "unknown")
            parts = message.get("content", {}).get("parts", [])
            content = " ".join([p for p in parts if isinstance(p, str)]).strip()
            if not content:
                continue

            ts = message.get("create_time", None)
            dt_str = datetime.fromtimestamp(ts).isoformat() if ts else None

            if role == "user":
                messages.append(f"**You said** *(on {dt_str})*:\n\n{content}")
            elif role == "assistant":
                messages.append(f"**ChatGPT said** *(on {dt_str})*:\n\n{content}")
            else:
                messages.append(f"*{role.capitalize()}* *(on {dt_str})*:\n\n{content}")

        # Use the first message with a create_time to populate earliest_ts
        first_with_time = next((m for m in ordered if m.get("create_time")), None)
        earliest_ts = (
            datetime.fromtimestamp(first_with_time["create_time"]).isoformat()
            if first_with_time
            else None
        )

        full_text = "\n\n".join(messages)
        if full_text.strip():
            documents.append(full_text)
            metadatas.append(
                {
                    "title": conv.get("title", f"Chat {idx}"),
                    "id": conv.get("id"),
                    "source": "json",
                    "earliest_ts": earliest_ts,
                }
            )
            ids.append(f"chat-{idx}")

    print(f"Embedding {len(documents)} chats...")
    embeddings = embedder.encode(documents, show_progress_bar=True)

    print("Adding to ChromaDB...")
    collection.add(
        documents=documents,
        embeddings=embeddings.tolist(),
        metadatas=metadatas,
        ids=ids,
    )
    print("‚úÖ Chat indexing complete!")


@app.command()
def index_docs(
    doc_folder: str = typer.Argument(..., help="Folder containing .docx files")
):
    documents, metadatas, ids = [], [], []

    def format_paragraphs(doc):
        lines = []
        for p in doc.paragraphs:
            text = p.text.strip()
            if not text:
                continue
            if re.match(r"^(You said|ChatGPT said):?$", text):
                lines.append("")
                lines.append(f"**{text}**")
                lines.append("")
            else:
                lines.append(text)
        return "\n\n".join(lines)

    for idx, filename in enumerate(os.listdir(doc_folder)):
        full_path = os.path.join(doc_folder, filename)
        if (
            filename.endswith(".docx")
            and not filename.startswith("~$")
            and os.path.isfile(full_path)
        ):
            doc = Document(full_path)
            full_text = format_paragraphs(doc)
            if full_text.strip():
                documents.append(full_text)
                metadatas.append({"title": filename, "source": "docx"})
                ids.append(f"docx-{idx}")

    print(f"Embedding {len(documents)} documents...")
    embeddings = embedder.encode(documents, show_progress_bar=True)

    print("Adding to ChromaDB...")
    collection.add(
        documents=documents,
        embeddings=embeddings.tolist(),
        metadatas=metadatas,
        ids=ids,
    )
    print("‚úÖ Word document indexing complete!")


if __name__ == "__main__":
    app()
