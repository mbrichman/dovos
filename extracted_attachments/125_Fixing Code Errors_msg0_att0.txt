import json
import os
import typer
import sys
from typing import Optional, List, Tuple, Dict, Any
from docx import Document
from sentence_transformers import SentenceTransformer
import chromadb
import re
from datetime import datetime, timedelta
import tempfile
import markdown
import logging
import hashlib
import shutil
from pathlib import Path
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel
from rich.table import Table
from rich.markdown import Markdown
import concurrent.futures

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Rich console for better output
console = Console()

app = typer.Typer(help="Semantic search tool for ChatGPT conversations with sync capability")

# === CONFIG ===
COLLECTION_NAME = "chat_history"
PERSIST_DIR = "./chroma_storage"
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"
# Store metadata to track conversation state
META_DB_PATH = "./metadata_store.json"


# Create a class to handle all database operations
class ChatArchive:
    def __init__(self):
        self.embedder = None
        self.chroma_client = None
        self.collection = None
        self.metadata_store = {}
        self._initialize()
        self._load_metadata()

    def _initialize(self):
        """Initialize embedding model and database connection"""
        # Initialize chromadb client
        self.chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)

        # Initialize collection
        self.collection = self.chroma_client.get_or_create_collection(
            COLLECTION_NAME,
            metadata={
                "description": "ChatGPT conversation history with timestamp indexing and sync"
            },
        )

    def _load_metadata(self):
        """Load conversation metadata store"""
        if os.path.exists(META_DB_PATH):
            try:
                with open(META_DB_PATH, "r", encoding="utf-8") as f:
                    self.metadata_store = json.load(f)
                logger.info(f"Loaded metadata for {len(self.metadata_store)} conversations")
            except json.JSONDecodeError:
                console.print("[bold red]Error loading metadata store. Creating new one.[/]")
                self.metadata_store = {}
        else:
            self.metadata_store = {}

    def _save_metadata(self):
        """Save conversation metadata store"""
        with open(META_DB_PATH, "w", encoding="utf-8") as f:
            json.dump(self.metadata_store, f, indent=2)
        logger.info(f"Saved metadata for {len(self.metadata_store)} conversations")

    def load_embedder(self):
        """Load the embedding model (done lazily to save resources)"""
        if self.embedder is None:
            with console.status("[bold green]Loading embedding model..."):
                self.embedder = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)
        return self.embedder

    def get_count(self):
        """Get the number of documents in the collection"""
        return self.collection.count()

    def delete_collection(self):
        """Delete the collection"""
        self.chroma_client.delete_collection(COLLECTION_NAME)
        # Reinitialize
        self.collection = self.chroma_client.create_collection(COLLECTION_NAME)
        # Clear metadata
        self.metadata_store = {}
        self._save_metadata()

    def add_documents(self, documents, embeddings, metadatas, ids):
        """Add documents to the collection"""
        self.collection.add(
            documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids
        )

    def get_doc_by_id(self, doc_id):
        """Get a document by its ID"""
        try:
            result = self.collection.get(ids=[doc_id])
            if result["documents"]:
                return {
                    "document": result["documents"][0],
                    "metadata": result["metadatas"][0]
                }
            return None
        except Exception as e:
            logger.error(f"Error retrieving document {doc_id}: {str(e)}")
            return None

    def update_document(self, doc_id, document, embedding, metadata):
        """Update an existing document"""
        self.collection.update(
            ids=[doc_id],
            documents=[document],
            embeddings=[embedding],
            metadatas=[metadata]
        )

    def delete_document(self, doc_id):
        """Delete a document by its ID"""
        self.collection.delete(ids=[doc_id])

    def query(self, query_embeddings, n_results=5, where=None):
        """Query the collection"""
        if where:
            return self.collection.query(
                query_embeddings=query_embeddings, n_results=n_results, where=where
            )
        else:
            return self.collection.query(
                query_embeddings=query_embeddings, n_results=n_results
            )

    def get_documents(self, where=None, include=None, limit=None):
        """Get documents from the collection"""
        kwargs = {}
        if where:
            kwargs["where"] = where
        if include:
            kwargs["include"] = include
        if limit:
            kwargs["limit"] = limit

        return self.collection.get(**kwargs)
    
    def get_conversation_hash(self, conversation):
        """Generate a hash to detect conversation changes"""
        hash_data = json.dumps(conversation, sort_keys=True)
        return hashlib.md5(hash_data.encode('utf-8')).hexdigest()
    
    def backup_db(self, backup_dir="./backups"):
        """Create a backup of the current database"""
        os.makedirs(backup_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = os.path.join(backup_dir, f"chroma_backup_{timestamp}")
        os.makedirs(backup_path, exist_ok=True)
        
        # Backup Chroma DB
        if os.path.exists(PERSIST_DIR):
            for item in os.listdir(PERSIST_DIR):
                src_path = os.path.join(PERSIST_DIR, item)
                dst_path = os.path.join(backup_path, item)
                if os.path.isdir(src_path):
                    shutil.copytree(src_path, dst_path)
                else:
                    shutil.copy2(src_path, dst_path)
        
        # Backup metadata
        if os.path.exists(META_DB_PATH):
            shutil.copy2(META_DB_PATH, os.path.join(backup_path, "metadata_store.json"))
        
        console.print(f"[bold green]Created backup at {backup_path}[/]")
        return backup_path


# Create global archive instance
archive = ChatArchive()


def format_timestamp(ts):
    """Format a Unix timestamp into a human-readable string"""
    if not ts:
        return "unknown time"
    try:
        return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")
    except (TypeError, ValueError):
        return "invalid time"


def parse_date_range(date_range: str):
    """Parse a date range string into start and end timestamps"""
    now = datetime.now()

    if date_range.lower() == "last week":
        start_date = now - timedelta(days=7)
        end_date = now
    elif date_range.lower() == "last month":
        start_date = now - timedelta(days=30)
        end_date = now
    elif date_range.lower() == "last year":
        start_date = now - timedelta(days=365)
        end_date = now
    elif " to " in date_range:
        start_str, end_str = date_range.split(" to ")
        try:
            start_date = datetime.fromisoformat(start_str)
            end_date = datetime.fromisoformat(end_str)
            # Set end date to end of day
            end_date = end_date.replace(hour=23, minute=59, second=59)
        except ValueError:
            console.print("[bold red]Invalid date format. Use YYYY-MM-DD format.[/]")
            return None, None
    else:
        # Single date
        try:
            start_date = datetime.fromisoformat(date_range)
            end_date = start_date.replace(hour=23, minute=59, second=59)
        except ValueError:
            console.print("[bold red]Invalid date format. Use YYYY-MM-DD format.[/]")
            return None, None

    return start_date.isoformat(), end_date.isoformat()


@app.command()
def search(
    query: str = typer.Argument(..., help="Search query"),
    top_k: int = typer.Option(5, "--top", "-t", help="Number of results to return"),
    keyword: bool = typer.Option(
        False,
        "--keyword",
        "-k",
        help="Use literal keyword search instead of semantic search",
    ),
    date_range: Optional[str] = typer.Option(
        None,
        "--date",
        "-d",
        help="Filter by date range (e.g., '2023-01-01 to 2023-01-31' or 'last week')",
    ),
    export: bool = typer.Option(
        False,
        "--export",
        "-e",
        help="Export results to markdown files",
    ),
):
    """Search through indexed conversations semantically or by keywords"""
    console.print(f"[bold]üîç Running search...[/]")
    doc_count = archive.get_count()
    console.print(f"[bold]üìÑ Document count in DB:[/] {doc_count}")

    if doc_count == 0:
        console.print(
            "[bold red]No documents found in the database. Run the 'index' command first.[/]"
        )
        raise typer.Exit(code=1)

    # Handle date filtering
    date_filter = None
    if date_range:
        start_date, end_date = parse_date_range(date_range)
        if start_date and end_date:
            console.print(
                f"[bold]üìÖ Filtering by date range:[/] {start_date} to {end_date}"
            )
            date_filter = {
                "$and": [
                    {"earliest_ts": {"$gte": start_date}},
                    {"latest_ts": {"$lte": end_date}},
                ]
            }

    results = {"documents": [[]], "metadatas": [[]]}

    if keyword:
        console.print("[bold]üîé Using keyword-based search.[/]")
        # Get all docs matching date filter if specified
        where_filter = date_filter if date_filter else None
        all_docs = archive.get_documents(
            where=where_filter, include=["documents", "metadatas"], limit=9999
        )
        terms = (
            query.replace("AND", "&&").replace("OR", "||").replace("NOT", "!!").split()
        )
        matches = []

        def match(doc):
            text = doc.lower()
            expr = ""
            for term in terms:
                if term == "&&":
                    expr += " and "
                elif term == "||":
                    expr += " or "
                elif term == "!!":
                    expr += " not "
                else:
                    expr += f"'{term.lower()}' in text"
            try:
                return eval(expr)
            except Exception:
                return False

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing conversations..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(conversations))

        for idx, conv in enumerate(conversations):
            progress.update(task, advance=1)

            messages = []
            message_map = conv.get("mapping", {})
            ordered = sorted(
                message_map.values(), key=lambda m: m.get("create_time", 0)
            )

            # Store all timestamps for more comprehensive filtering
            timestamps = []

            for msg in ordered:
                message = msg.get("message")
                if not message:
                    continue

                role = message.get("author", {}).get("role", "unknown")
                parts = message.get("content", {}).get("parts", [])
                content = " ".join([p for p in parts if isinstance(p, str)]).strip()
                if not content:
                    continue

                ts = message.get("create_time", None)
                if ts:
                    timestamps.append(ts)
                dt_str = format_timestamp(ts)

                if role == "user":
                    messages.append(f"**You said** *(on {dt_str})*:\n\n{content}")
                elif role == "assistant":
                    messages.append(f"**ChatGPT said** *(on {dt_str})*:\n\n{content}")
                else:
                    messages.append(
                        f"*{role.capitalize()}* *(on {dt_str})*:\n\n{content}"
                    )

            # Calculate earliest and latest timestamps
            valid_timestamps = [ts for ts in timestamps if ts]
            earliest_ts = min(valid_timestamps) if valid_timestamps else None
            latest_ts = max(valid_timestamps) if valid_timestamps else None

            # Convert timestamps to ISO format for better filtering
            earliest_ts_iso = (
                datetime.fromtimestamp(earliest_ts).isoformat() if earliest_ts else None
            )
            latest_ts_iso = (
                datetime.fromtimestamp(latest_ts).isoformat() if latest_ts else None
            )

            full_text = "\n\n".join(messages)

            # Skip empty conversations
            if not full_text.strip():
                continue

            # Handle chunking if enabled
            if chunk_size > 0 and len(messages) > chunk_size:
                chunks = [
                    messages[i : i + chunk_size]
                    for i in range(0, len(messages), chunk_size)
                ]

                for chunk_idx, chunk in enumerate(chunks):
                    chunk_text = "\n\n".join(chunk)
                    chunk_title = f"{conv.get('title', f'Chat {idx}')} (Part {chunk_idx+1}/{len(chunks)})"

                    documents.append(chunk_text)

                    # Create a metadata dict with no None values
                    metadata_dict = {
                        "title": chunk_title,
                        "source": "json",
                        "message_count": len(chunk),
                        "is_chunk": True,
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks),
                    }

                    # Only add fields that aren't None
                    if conv.get("id"):
                        metadata_dict["id"] = f"{conv.get('id')}-{chunk_idx}"
                        metadata_dict["conversation_id"] = conv.get("id")
                    if earliest_ts_iso:
                        metadata_dict["earliest_ts"] = earliest_ts_iso
                    if latest_ts_iso:
                        metadata_dict["latest_ts"] = latest_ts_iso

                    metadatas.append(metadata_dict)
                    ids.append(f"chat-{idx}-chunk-{chunk_idx}")
            else:
                documents.append(full_text)

                # Create a metadata dict with no None values
                metadata_dict = {
                    "title": conv.get("title", f"Chat {idx}"),
                    "source": "json",
                    "message_count": len(messages),
                    "is_chunk": False,
                }

                # Only add fields that aren't None
                if conv.get("id"):
                    metadata_dict["id"] = conv.get("id")
                if earliest_ts_iso:
                    metadata_dict["earliest_ts"] = earliest_ts_iso
                if latest_ts_iso:
                    metadata_dict["latest_ts"] = latest_ts_iso

                metadatas.append(metadata_dict)
                ids.append(f"chat-{idx}")

    if not documents:
        console.print("[bold red]No valid conversations found to index.[/]")
        return

    console.print(f"[bold]Embedding {len(documents)} documents...[/]")

    # Process in batches to avoid memory issues with large datasets
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        end = min(i + batch_size, len(documents))

        with console.status(
            f"[bold green]Embedding batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}..."
        ):
            batch_docs = documents[i:end]
            batch_metas = metadatas[i:end]
            batch_ids = ids[i:end]

            embeddings = embedder.encode(batch_docs, show_progress_bar=False)

            archive.add_documents(
                documents=batch_docs,
                embeddings=embeddings.tolist(),
                metadatas=batch_metas,
                ids=batch_ids,
            )

    console.print("[bold green]‚úÖ Chat indexing complete![/]")
    console.print(f"[bold]Total documents indexed: {len(documents)}[/]")


@app.command()
def index_docs(
    doc_folder: str = typer.Argument(..., help="Folder containing .docx files"),
    overwrite: bool = typer.Option(
        False,
        "--overwrite",
        "-o",
        help="Overwrite existing documents from the same source",
    ),
    maintain_local: bool = typer.Option(
        True,
        "--maintain-local/--full-replace",
        help="Maintain local documents not present in the folder",
    ),
):
    """Index Word documents containing chat conversations while preserving local content"""
    # Create a backup before proceeding
    if not maintain_local or overwrite:
        backup_path = archive.backup_db()
        console.print(f"[bold green]Created backup at: {backup_path}[/]")
        
    # Get the file paths of all .docx files in the folder
    folder_path = Path(doc_folder)
    if not folder_path.exists() or not folder_path.is_dir():
        console.print(f"[bold red]Error: Folder '{doc_folder}' not found or not a directory.[/]")
        return
        
    docx_files = list(folder_path.glob("*.docx"))
    docx_files = [f for f in docx_files if not f.name.startswith("~$")]
    
    if not docx_files:
        console.print(f"[bold red]No .docx files found in '{doc_folder}'.[/]")
        return
    
    console.print(f"[bold]Found {len(docx_files)} Word documents.[/]")
    
    # Get existing DOCX documents 
    existing_docs = archive.get_documents(
        where={"source": "docx"}, include=["metadatas", "ids"]
    )
    
    # Map file paths to document IDs for quick lookup
    existing_file_to_id = {}
    for i, meta in enumerate(existing_docs.get("metadatas", [])):
        if "file_path" in meta:
            existing_file_to_id[meta["file_path"]] = existing_docs["ids"][i]
    
    # Track which files we've seen for the delete step
    processed_ids = set()
    
    if overwrite:
        # Delete existing documents from the 'docx' source
        if existing_docs["metadatas"]:
            existing_ids = existing_docs["ids"]
            archive.collection.delete(ids=existing_ids)
            console.print(
                f"[bold]Deleted {len(existing_ids)} existing Word documents.[/]"
            )

    # Load the embedder
    embedder = archive.load_embedder()

    documents, metadatas, ids = [], [], []
    stats = {
        "new": 0,
        "updated": 0,
        "unchanged": 0,
        "preserved": 0
    }

    def extract_timestamp(text):
        """Try to extract timestamps from text using regex"""
        # Look for common date formats
        date_patterns = [
            r"(\d{4}-\d{2}-\d{2})",  # YYYY-MM-DD
            r"(\d{2}/\d{2}/\d{4})",  # MM/DD/YYYY
            r"(\w+ \d{1,2}, \d{4})",  # Month DD, YYYY
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                try:
                    # Try different formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y"):
                        try:
                            dt = datetime.strptime(matches[0], fmt)
                            return dt.isoformat()
                        except ValueError:
                            continue
                except Exception:
                    pass
        return None
        
    def calculate_file_hash(file_path):
        """Calculate a hash of the file contents"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
        
    def format_paragraphs(doc):
        lines = []
        timestamps = []
        current_role = None
        current_content = []

        for p in doc.paragraphs:
            text = p.text.strip()
            if not text:
                if current_role and current_content:
                    formatted_content = "\n".join(current_content)
                    lines.append(f"**{current_role}**:\n\n{formatted_content}\n")
                    current_content = []
                continue

            # Extract potential timestamp
            ts = extract_timestamp(text)
            if ts:
                timestamps.append(ts)

            # Check if this is a role indicator
            role_match = re.match(
                r"^(You|ChatGPT|Claude|User|Assistant|System|AI)(\s+said)?:?$",
                text,
                re.IGNORECASE,
            )
            if role_match:
                # Save previous content if any
                if current_role and current_content:
                    formatted_content = "\n".join(current_content)
                    lines.append(f"**{current_role}**:\n\n{formatted_content}\n")

                current_role = role_match.group(1)
                current_content = []
            else:
                # Add to current content
                if current_role:
                    current_content.append(text)
                else:
                    lines.append(text)

        # Don't forget the last block
        if current_role and current_content:
            formatted_content = "\n".join(current_content)
            lines.append(f"**{current_role}**:\n\n{formatted_content}\n")

        return "\n".join(lines), timestamps

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing Word documents..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(docx_files))

        for idx, file_path in enumerate(docx_files):
            progress.update(task, advance=1)
            
            full_path = str(file_path.absolute())
            file_hash = calculate_file_hash(full_path)
            
            # Check if this file already exists in our database
            existing_id = existing_file_to_id.get(full_path)
            is_update = False
            
            # If not overwriting, check if file has changed
            if not overwrite and existing_id:
                # Get metadata
                doc_info = archive.get_doc_by_id(existing_id)
                if doc_info and doc_info["metadata"].get("file_hash") == file_hash:
                    # File hasn't changed, skip processing
                    processed_ids.add(existing_id)
                    stats["unchanged"] += 1
                    progress.update(task, description=f"[dim]Skipping unchanged file: {file_path.name}")
                    continue
                else:
                    # File has changed, mark for update
                    is_update = True
            
            try:
                doc = Document(full_path)
                full_text, timestamps = format_paragraphs(doc)

                if not full_text.strip():
                    continue

                # Get file modification time as fallback timestamp
                file_mod_time = datetime.fromtimestamp(
                    os.path.getmtime(full_path)
                ).isoformat()

                # Use extracted timestamps if available, otherwise use file timestamps
                earliest_ts = min(timestamps) if timestamps else file_mod_time
                latest_ts = max(timestamps) if timestamps else file_mod_time

                # Create a unique ID based on file path
                doc_id = f"docx-{hashlib.md5(full_path.encode()).hexdigest()}"
                
                # Create metadata with no None values
                metadata_dict = {
                    "title": file_path.name,
                    "source": "docx",
                    "file_path": full_path,
                    "file_hash": file_hash,
                    "last_indexed": datetime.now().isoformat()
                }

                # Only add timestamps if they exist
                if earliest_ts:
                    metadata_dict["earliest_ts"] = earliest_ts
                if latest_ts:
                    metadata_dict["latest_ts"] = latest_ts
                
                # Calculate embedding
                embedding = embedder.encode(full_text, show_progress_bar=False)
                
                if is_update:
                    # Update existing document
                    archive.update_document(
                        doc_id=existing_id,
                        document=full_text,
                        embedding=embedding.tolist(),
                        metadata=metadata_dict
                    )
                    processed_ids.add(existing_id)
                    stats["updated"] += 1
                    progress.update(task, description=f"[yellow]Updated: {file_path.name}")
                else:
                    # Add new document
                    archive.add_documents(
                        documents=[full_text],
                        embeddings=[embedding.tolist()],
                        metadatas=[metadata_dict],
                        ids=[doc_id]
                    )
                    processed_ids.add(doc_id)
                    stats["new"] += 1
                    progress.update(task, description=f"[green]Added: {file_path.name}")

            except Exception as e:
                console.print(f"[bold red]Error processing {file_path.name}: {str(e)}[/]")

    # If not overwriting and maintaining local content, handle the documents that weren't seen
    if maintain_local and not overwrite:
        unprocessed_ids = set(existing_docs.get("ids", [])) - processed_ids
        if unprocessed_ids:
            stats["preserved"] = len(unprocessed_ids)
            console.print(f"[bold blue]Preserving {len(unprocessed_ids)} documents not found in the current folder.[/]")
    # Otherwise, if not maintaining local content, delete any documents not processed
    elif not maintain_local and not overwrite:
        unprocessed_ids = set(existing_docs.get("ids", [])) - processed_ids
        if unprocessed_ids:
            console.print(f"[bold red]Removing {len(unprocessed_ids)} documents not found in the current folder.[/]")
            for doc_id in unprocessed_ids:
                archive.delete_document(doc_id)

    # Print summary
    console.print("\n[bold]Document Processing Summary:[/]")
    table = Table()
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")
    table.add_row("New documents", str(stats["new"]))
    table.add_row("Updated documents", str(stats["updated"]))
    table.add_row("Unchanged documents", str(stats["unchanged"]))
    table.add_row("Preserved local documents", str(stats["preserved"]))
    console.print(table)

    console.print("[bold green]‚úÖ Word document indexing complete![/]")
    console.print(f"[bold]Total active documents in database: {archive.get_count()}[/]")


@app.command()
def stats():
    """Show statistics about the indexed data"""
    doc_count = archive.get_count()
    if doc_count == 0:
        console.print(
            "[bold red]No documents found in the database. Run the 'index' command first.[/]"
        )
        return

    console.print(f"[bold]üìä Collection Statistics[/]")
    console.print(f"[bold]Total documents:[/] {doc_count}")

    # Get all metadata to analyze
    all_meta = archive.get_documents(include=["metadatas"], limit=9999)["metadatas"]

    # Count by source
    sources = {}
    for meta in all_meta:
        source = meta.get("source", "unknown")
        sources[source] = sources.get(source, 0) + 1

    console.print("\n[bold]Documents by source:[/]")
    for source, count in sources.items():
        console.print(f"  {source}: {count}")

    # Get date range
    dates = [meta.get("earliest_ts") for meta in all_meta if meta.get("earliest_ts")]
    if dates:
        dates.sort()
        console.print(f"\n[bold]Date range:[/] {dates[0]} to {dates[-1]}")

    # Count by chunks
    chunked = sum(1 for meta in all_meta if meta.get("is_chunk", False))
    console.print(f"\n[bold]Chunked documents:[/] {chunked}")
    console.print(f"[bold]Full documents:[/] {doc_count - chunked}")


@app.command()
def backup():
    """Create a backup of the current database"""
    backup_path = archive.backup_db()
    console.print(f"[bold green]‚úÖ Backup created at: {backup_path}[/]")


@app.command()
def restore(
    backup_path: str = typer.Argument(..., help="Path to the backup directory"),
):
    """Restore the database from a backup"""
    if not os.path.exists(backup_path):
        console.print(f"[bold red]Error: Backup path '{backup_path}' not found.[/]")
        return

    if not os.path.exists(os.path.join(backup_path, "metadata_store.json")):
        console.print(f"[bold red]Error: Invalid backup directory. Missing metadata store.[/]")
        return

    # Confirm with user
    if not typer.confirm(f"This will overwrite your current database with the backup at {backup_path}. Continue?"):
        console.print("[bold]Operation cancelled.[/]")
        return

    # Create a backup of the current state just in case
    current_backup = archive.backup_db(backup_dir="./emergency_backups")
    console.print(f"[bold]Created emergency backup at: {current_backup}[/]")

    # Close the current database connection
    try:
        archive.collection = None
        archive.chroma_client = None
    except Exception:
        pass

    # Restore from backup
    try:
        # Delete current DB
        if os.path.exists(PERSIST_DIR):
            shutil.rmtree(PERSIST_DIR)
            
        # Copy backup to current location
        shutil.copytree(backup_path, PERSIST_DIR, dirs_exist_ok=True)
        
        # Restore metadata
        backup_meta_path = os.path.join(backup_path, "metadata_store.json")
        if os.path.exists(backup_meta_path):
            shutil.copy2(backup_meta_path, META_DB_PATH)
        
        # Reinitialize
        archive._initialize()
        archive._load_metadata()
        
        console.print(f"[bold green]‚úÖ Successfully restored from backup.[/]")
        console.print(f"[bold]Document count: {archive.get_count()}[/]")
        
    except Exception as e:
        console.print(f"[bold red]Error during restore: {str(e)}[/]")
        console.print(f"[bold]You can try to manually restore from the emergency backup at {current_backup}[/]")


@app.command()
def clear():
    """Clear all indexed data"""
    if typer.confirm("Are you sure you want to delete all indexed data?"):
        # Create a backup before deletion
        backup_path = archive.backup_db(backup_dir="./deletion_backups")
        console.print(f"[bold]Created safety backup at: {backup_path}[/]")
        
        archive.delete_collection()
        console.print("[bold red]Collection deleted.[/]")
    else:
        console.print("[bold]Operation cancelled.[/]")


@app.command()
def export(
    output_dir: str = typer.Argument(..., help="Directory to export data to"),
    format: str = typer.Option(
        "md",
        "--format",
        "-f",
        help="Export format: md (Markdown) or json (JSON)",
    ),
    where_json: Optional[str] = typer.Option(
        None,
        "--where",
        "-w",
        help="Where filter in JSON format (e.g., '{\"source\": \"json\"}')",
    ),
):
    """Export indexed conversations to files"""
    os.makedirs(output_dir, exist_ok=True)
    
    # Parse where filter if provided
    where_filter = None
    if where_json:
        try:
            where_filter = json.loads(where_json)
        except json.JSONDecodeError:
            console.print("[bold red]Error: Invalid JSON in where filter.[/]")
            return
    
    # Get all documents matching the filter
    result = archive.get_documents(where=where_filter, include=["documents", "metadatas"])
    
    if not result["documents"]:
        console.print("[bold red]No documents found matching the criteria.[/]")
        return
    
    console.print(f"[bold]Exporting {len(result['documents'])} documents...[/]")
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Exporting documents..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Exporting", total=len(result["documents"]))
        
        if format.lower() == "md":
            # Export as individual markdown files
            for idx, (doc, meta) in enumerate(zip(result["documents"], result["metadatas"])):
                progress.update(task, advance=1)
                
                # Create a clean filename
                title = meta.get("title", f"document_{idx}")
                clean_title = re.sub(r'[^\w\s-]', '', title).strip().replace(' ', '_')
                filename = f"{clean_title}_{idx}.md"
                file_path = os.path.join(output_dir, filename)
                
                with open(file_path, "w", encoding="utf-8") as f:
                    # Write metadata as YAML front matter
                    f.write("---\n")
                    for key, value in meta.items():
                        if value is not None:
                            f.write(f"{key}: {value}\n")
                    f.write("---\n\n")
                    
                    # Write the document content
                    f.write(f"# {title}\n\n")
                    f.write(doc)
        
        elif format.lower() == "json":
            # Export as a single JSON file
            export_data = {
                "documents": [],
                "timestamp": datetime.now().isoformat(),
                "document_count": len(result["documents"])
            }
            
            for idx, (doc, meta) in enumerate(zip(result["documents"], result["metadatas"])):
                progress.update(task, advance=1)
                
                export_data["documents"].append({
                    "id": idx,
                    "content": doc,
                    "metadata": meta
                })
            
            file_path = os.path.join(output_dir, f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2)
        
        else:
            console.print(f"[bold red]Unsupported export format: {format}[/]")
            return
    
    console.print(f"[bold green]‚úÖ Successfully exported documents to: {output_dir}[/]")


@app.command()
def cleanup(
    max_age_days: int = typer.Option(
        90,
        "--max-age",
        "-m",
        help="Remove documents older than this many days",
    ),
    dry_run: bool = typer.Option(
        True,
        "--dry-run/--execute",
        help="Show what would be removed without actually removing",
    ),
):
    """Clean up old documents from the database"""
    cutoff_date = (datetime.now() - timedelta(days=max_age_days)).isoformat()
    
    # Find documents older than cutoff
    where_filter = {"latest_ts": {"$lt": cutoff_date}}
    old_docs = archive.get_documents(where=where_filter, include=["metadatas", "ids"])
    
    if not old_docs["metadatas"]:
        console.print(f"[bold]No documents found older than {max_age_days} days.[/]")
        return
    
    console.print(f"[bold]Found {len(old_docs['metadatas'])} documents older than {max_age_days} days.[/]")
    
    # Show document details
    table = Table()
    table.add_column("ID", style="cyan")
    table.add_column("Title", style="green")
    table.add_column("Source", style="blue")
    table.add_column("Date", style="yellow")
    
    for idx, (doc_id, meta) in enumerate(zip(old_docs["ids"], old_docs["metadatas"])):
        if idx < 10:  # Only show first 10 for brevity
            table.add_row(
                doc_id[:10] + "...",
                meta.get("title", "Untitled")[:30],
                meta.get("source", "unknown"),
                meta.get("earliest_ts", "Unknown")
            )
    
    if len(old_docs["metadatas"]) > 10:
        table.add_row("...", "...", "...", "...")
    
    console.print(table)
    
    if dry_run:
        console.print("[bold yellow]This was a dry run. Run with --execute to actually remove these documents.[/]")
    else:
        if typer.confirm(f"Are you sure you want to delete these {len(old_docs['metadatas'])} old documents?"):
            # Create backup first
            backup_path = archive.backup_db(backup_dir="./cleanup_backups")
            console.print(f"[bold]Created backup at: {backup_path}[/]")
            
            # Delete documents
            archive.collection.delete(ids=old_docs["ids"])
            console.print(f"[bold green]‚úÖ Successfully removed {len(old_docs['ids'])} old documents.[/]")
            console.print(f"[bold]Documents remaining: {archive.get_count()}[/]")
        else:
            console.print("[bold]Operation cancelled.[/]")


@app.command()
def version():
    """Show version information and system stats"""
    console.print(f"[bold]üìä ChatArchive Tool[/]")
    console.print(f"[bold]Version:[/] 2.0.0")
    console.print(f"[bold]Database path:[/] {os.path.abspath(PERSIST_DIR)}")
    console.print(f"[bold]Embedding model:[/] {DEFAULT_EMBEDDING_MODEL}")
    console.print(f"[bold]Document count:[/] {archive.get_count()}")
    console.print(f"[bold]Python version:[/] {'.'.join(map(str, tuple(sys.version_info)[:3]))}")
    
    # Database storage usage
    db_size = 0
    if os.path.exists(PERSIST_DIR):
        for path, dirs, files in os.walk(PERSIST_DIR):
            for f in files:
                fp = os.path.join(path, f)
                db_size += os.path.getsize(fp)
    
    console.print(f"[bold]Database size:[/] {db_size / (1024*1024):.2f} MB")
    
    # System info
    console.print(f"[bold]CPU count:[/] {os.cpu_count()}")
    
    # Get available RAM
    try:
        import psutil
        ram = psutil.virtual_memory()
        console.print(f"[bold]Available RAM:[/] {ram.available / (1024*1024*1024):.2f} GB / {ram.total / (1024*1024*1024):.2f} GB")
    except ImportError:
        console.print("[dim]Install psutil for memory information[/]")


if __name__ == "__main__":
    import sys
    try:
        app()

        # Now start a new block with the Progress context manager
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold green]Processing keyword search..."),
            transient=True,
        ) as progress:
            task = progress.add_task("Searching", total=len(all_docs["documents"]))
            for idx, doc in enumerate(all_docs["documents"]):
                if match(doc):
                    matches.append((doc, all_docs["metadatas"][idx]))
                progress.update(task, advance=1)

            if not matches:
                console.print("[bold red]‚ùå No keyword matches found.[/]")
                sys.exit(0)

            console.print(f"[bold green]‚úÖ Found {len(matches)} keyword matches.[/]")
            for idx, (doc, meta) in enumerate(matches[:top_k]):
                console.print(
                    f"\n[bold cyan]Result {idx+1}:[/] {meta.get('title', 'Untitled')}"
                )
                console.print(f"[dim]Date: {meta.get('earliest_ts', 'Unknown')}[/]")
                console.print(doc[:300] + "...\n")
                results["documents"][0].append(doc)
                results["metadatas"][0].append(meta)

    else:
        # Vector search
        # Load embedder for semantic search
        embedder = archive.load_embedder()

        with console.status("[bold green]Running semantic search..."):
            embedding = embedder.encode([query])[0]

            # Perform the query with date filter if specified
            results = archive.query(
                query_embeddings=[embedding.tolist()],
                n_results=top_k,
                where=date_filter,
            )

        if not results["documents"][0]:
            console.print("[bold red]‚ùå No semantic matches found.[/]")
            return

        for idx, (doc, meta) in enumerate(
            zip(results["documents"][0], results["metadatas"][0])
        ):
            console.print(
                f"\n[bold cyan]Result {idx+1}:[/] {meta.get('title', 'Untitled')}"
            )
            console.print(f"[dim]Date: {meta.get('earliest_ts', 'Unknown')}[/]")
            console.print(doc[:300] + "...\n")

    # Export results if requested
    if export:
        export_dir = "search_results"
        os.makedirs(export_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for idx, (doc, meta) in enumerate(
            zip(results["documents"][0], results["metadatas"][0])
        ):
            title = meta.get("title", f"Result_{idx}").replace(" ", "_")[:30]
            filename = f"{export_dir}/{timestamp}_{title}.md"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"# {meta.get('title', 'Untitled')}\n\n")
                f.write(f"Date: {meta.get('earliest_ts', 'Unknown')}\n\n")
                f.write(doc)

            console.print(f"[bold green]Saved result {idx+1} to {filename}[/]")

        raise typer.Exit()

    # Interactive result viewing
    try:
        choice = console.input(
            "\n[bold]Enter result number to view as HTML in Safari (or press Enter to skip):[/] "
        ).strip()
    except (EOFError, KeyboardInterrupt):
        console.print("\n[dim]‚è≠Ô∏è Skipped.[/]")
        return

    if not choice:
        return

    if choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(results["documents"][0]):
            full_text = results["documents"][0][idx]
            meta = results["metadatas"][0][idx]
            title = meta.get("title", "Untitled").replace(" ", "_")[:30]
            date_str = meta.get("earliest_ts", "Unknown")

            with open("debug_output.md", "w", encoding="utf-8") as f:
                f.write(full_text)

            html_body = markdown.markdown(
                full_text, extensions=["sane_lists", "tables", "nl2br"]
            )
            html_full = f"""
            <html><head><meta charset="UTF-8"><title>{title}</title>
            <style>
            body {{ font-family: Georgia, serif; max-width: 700px; margin: auto; line-height: 1.6; padding: 20px; }}
            h1 {{ color: #2c3e50; }}
            .metadata {{ color: #7f8c8d; margin-bottom: 20px; }}
            strong {{ color: #2980b9; }}
            pre {{ background-color: #f8f9fa; padding: 10px; border-radius: 5px; overflow-x: auto; }}
            code {{ font-family: 'Courier New', monospace; }}
            </style></head>
            <body>
            <h1>{meta.get('title', 'Untitled')}</h1>
            <div class="metadata">Date: {date_str}</div>
            {html_body}
            </body></html>"""

            with tempfile.NamedTemporaryFile(
                "w", delete=False, suffix=".html", encoding="utf-8"
            ) as tmp:
                tmp.write(html_full)
                tmp_path = tmp.name

            console.print(f"[bold green]Opening result in Safari...[/]")
            os.system(f'open -a Safari "{tmp_path}"')
        else:
            console.print("[bold red]‚ùå Invalid result number.[/]")
    else:
        console.print("[bold red]‚ùå Invalid input.[/]")


@app.command()
def sync(
    chat_json_path: str = typer.Argument(
        ..., help="Path to your conversations.json file"
    ),
    chunk_size: int = typer.Option(
        0,
        "--chunk",
        "-c",
        help="Split conversations into chunks of this size (0 for no chunking)",
    ),
    create_backup: bool = typer.Option(
        True,
        "--backup/--no-backup",
        help="Create a backup before synchronizing",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        "-d",
        help="Simulate sync without making changes",
    ),
):
    """Synchronize ChatGPT conversations while preserving locally archived chats"""
    doc_count = archive.get_count()
    console.print(f"[bold]Current archive has {doc_count} documents[/]")

    # Create backup first if requested
    if create_backup and not dry_run:
        backup_path = archive.backup_db()
        console.print(f"[bold green]Created backup at: {backup_path}[/]")

    try:
        with open(chat_json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        console.print("[bold red]Error: Invalid JSON file.[/]")
        return
    except FileNotFoundError:
        console.print(f"[bold red]Error: File '{chat_json_path}' not found.[/]")
        return

    # Load the embedder
    embedder = archive.load_embedder()

    conversations = data if isinstance(data, list) else data.get("conversations", [])
    console.print(
        f"[bold]Found {len(conversations)} conversations in the new JSON file.[/]"
    )

    # First pass: get all conversation IDs from the file
    new_ids = set()
    for conv in conversations:
        if conv.get("id"):
            new_ids.add(conv.get("id"))

    # Check existing conversations in our database
    existing_docs = archive.get_documents(include=["metadatas"])
    existing_conv_ids = set()
    
    for meta in existing_docs["metadatas"]:
        if meta.get("conversation_id"):
            existing_conv_ids.add(meta.get("conversation_id"))
        elif meta.get("id") and not meta.get("is_chunk", False):
            existing_conv_ids.add(meta.get("id"))
    
    # Calculate differences
    removed_ids = existing_conv_ids - new_ids
    new_or_updated_ids = new_ids
    
    stats = {
        "new": 0,
        "updated": 0,
        "removed_but_preserved": len(removed_ids),
        "chunks_processed": 0
    }
    
    console.print(f"[bold]Found {len(removed_ids)} conversations that were removed from ChatGPT but will be preserved locally[/]")
    
    if dry_run:
        console.print("[bold yellow]DRY RUN - no changes will be made[/]")
        
    documents, metadatas, ids = [], [], []

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing conversations..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(conversations))

        for idx, conv in enumerate(conversations):
            progress.update(task, advance=1)
            conv_id = conv.get("id")
            
            if not conv_id:
                continue  # Skip conversations without ID
                
            # Check if we already have this conversation
            conv_in_db = False
            conv_updated = False
            
            # Compare checksum if it exists in metadata store
            conv_hash = archive.get_conversation_hash(conv)
            if conv_id in archive.metadata_store:
                old_hash = archive.metadata_store[conv_id].get("hash")
                if old_hash == conv_hash:
                    # Conversation hasn't changed, skip processing
                    progress.update(task, description=f"[cyan]Skipping unchanged conversation {conv_id}")
                    continue
                else:
                    # Conversation has been modified
                    conv_in_db = True
                    conv_updated = True
            
            # Process the conversation
            messages = []
            message_map = conv.get("mapping", {})
            ordered = sorted(
                message_map.values(), key=lambda m: m.get("create_time", 0)
            )

            # Store all timestamps for more comprehensive filtering
            timestamps = []

            for msg in ordered:
                message = msg.get("message")
                if not message:
                    continue

                role = message.get("author", {}).get("role", "unknown")
                parts = message.get("content", {}).get("parts", [])
                content = " ".join([p for p in parts if isinstance(p, str)]).strip()
                if not content:
                    continue

                ts = message.get("create_time", None)
                if ts:
                    timestamps.append(ts)
                dt_str = format_timestamp(ts)

                if role == "user":
                    messages.append(f"**You said** *(on {dt_str})*:\n\n{content}")
                elif role == "assistant":
                    messages.append(f"**ChatGPT said** *(on {dt_str})*:\n\n{content}")
                else:
                    messages.append(
                        f"*{role.capitalize()}* *(on {dt_str})*:\n\n{content}"
                    )

            # Calculate earliest and latest timestamps
            valid_timestamps = [ts for ts in timestamps if ts]
            earliest_ts = min(valid_timestamps) if valid_timestamps else None
            latest_ts = max(valid_timestamps) if valid_timestamps else None

            # Convert timestamps to ISO format for better filtering
            earliest_ts_iso = (
                datetime.fromtimestamp(earliest_ts).isoformat() if earliest_ts else None
            )
            latest_ts_iso = (
                datetime.fromtimestamp(latest_ts).isoformat() if latest_ts else None
            )

            full_text = "\n\n".join(messages)

            # Skip empty conversations
            if not full_text.strip():
                continue

            # Update metadata store for this conversation
            if not dry_run:
                archive.metadata_store[conv_id] = {
                    "hash": conv_hash,
                    "last_updated": datetime.now().isoformat(),
                    "title": conv.get("title", f"Chat {idx}"),
                    "earliest_ts": earliest_ts_iso,
                    "latest_ts": latest_ts_iso,
                }

            # Handle chunking if enabled
            if chunk_size > 0 and len(messages) > chunk_size:
                chunks = [
                    messages[i : i + chunk_size]
                    for i in range(0, len(messages), chunk_size)
                ]

                # If conversation was updated, delete old chunks first
                if conv_updated and not dry_run:
                    # Get all chunks for this conversation
                    existing_chunks = archive.get_documents(
                        where={"conversation_id": conv_id}, include=["metadatas", "ids"]
                    )
                    for chunk_id in existing_chunks.get("ids", []):
                        archive.delete_document(chunk_id)
                    
                for chunk_idx, chunk in enumerate(chunks):
                    chunk_text = "\n\n".join(chunk)
                    chunk_title = f"{conv.get('title', f'Chat {idx}')} (Part {chunk_idx+1}/{len(chunks)})"
                    chunk_id = f"chat-{conv_id}-chunk-{chunk_idx}"

                    # Create a metadata dict with no None values
                    metadata_dict = {
                        "title": chunk_title,
                        "source": "json",
                        "message_count": len(chunk),
                        "is_chunk": True,
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks),
                        "conversation_id": conv_id,
                    }

                    # Only add fields that aren't None
                    if earliest_ts_iso:
                        metadata_dict["earliest_ts"] = earliest_ts_iso
                    if latest_ts_iso:
                        metadata_dict["latest_ts"] = latest_ts_iso

                    if not dry_run:
                        # Calculate embedding
                        embedding = embedder.encode(chunk_text, show_progress_bar=False)
                        # Add the document
                        archive.add_documents(
                            documents=[chunk_text],
                            embeddings=[embedding.tolist()],
                            metadatas=[metadata_dict],
                            ids=[chunk_id]
                        )
                    
                    stats["chunks_processed"] += 1
                
                if conv_updated:
                    stats["updated"] += 1
                    progress.update(task, description=f"[yellow]Updated chunked conversation {conv_id}")
                else:
                    stats["new"] += 1
                    progress.update(task, description=f"[green]Added new chunked conversation {conv_id}")
                
            else:
                # Not chunked conversation
                doc_id = f"chat-{conv_id}"
                
                # Create a metadata dict with no None values
                metadata_dict = {
                    "title": conv.get("title", f"Chat {idx}"),
                    "source": "json",
                    "message_count": len(messages),
                    "is_chunk": False,
                    "id": conv_id,
                }

                # Only add fields that aren't None
                if earliest_ts_iso:
                    metadata_dict["earliest_ts"] = earliest_ts_iso
                if latest_ts_iso:
                    metadata_dict["latest_ts"] = latest_ts_iso

                if not dry_run:
                    # Calculate embedding
                    embedding = embedder.encode(full_text, show_progress_bar=False)
                    
                    if conv_updated:
                        # Update existing document
                        archive.update_document(
                            doc_id=doc_id,
                            document=full_text,
                            embedding=embedding.tolist(),
                            metadata=metadata_dict
                        )
                        stats["updated"] += 1
                        progress.update(task, description=f"[yellow]Updated conversation {conv_id}")
                    else:
                        # Add new document
                        archive.add_documents(
                            documents=[full_text],
                            embeddings=[embedding.tolist()],
                            metadatas=[metadata_dict],
                            ids=[doc_id]
                        )
                        stats["new"] += 1
                        progress.update(task, description=f"[green]Added new conversation {conv_id}")

    if not dry_run:
        # Save metadata store
        archive._save_metadata()
    
    # Print summary
    console.print("\n[bold]Sync Summary:[/]")
    table = Table()
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")
    table.add_row("New conversations", str(stats["new"]))
    table.add_row("Updated conversations", str(stats["updated"]))
    table.add_row("Preserved local conversations", str(stats["removed_but_preserved"]))
    table.add_row("Chunks processed", str(stats["chunks_processed"]))
    console.print(table)
    
    if dry_run:
        console.print("[bold yellow]This was a dry run. No changes were made.[/]")
    else:
        console.print("[bold green]‚úÖ ChatGPT archive sync complete![/]")


@app.command()
def index(
    chat_json_path: str = typer.Argument(
        ..., help="Path to your conversations.json file"
    ),
    chunk_size: int = typer.Option(
        0,
        "--chunk",
        "-c",
        help="Split conversations into chunks of this size (0 for no chunking)",
    ),
    overwrite: bool = typer.Option(
        False,
        "--overwrite",
        "-o",
        help="Overwrite existing collection",
    ),
):
    """Index ChatGPT conversations from a JSON export (legacy method, use sync instead)"""
    console.print("[bold yellow]NOTE: This is the legacy indexing method. Consider using 'sync' instead for better preservation of local content.[/]")
    
    # Check if collection already exists and has documents
    doc_count = archive.get_count()
    if doc_count > 0 and not overwrite:
        console.print(
            f"[bold yellow]Collection already contains {doc_count} documents.[/]"
        )
        if not typer.confirm("Do you want to add to the existing collection?"):
            if typer.confirm(
                "Do you want to delete the existing collection and start fresh?"
            ):
                console.print("[bold red]Deleting existing collection...[/]")
                archive.delete_collection()
            else:
                console.print("[bold]Operation cancelled.[/]")
                return

    try:
        with open(chat_json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        console.print("[bold red]Error: Invalid JSON file.[/]")
        return
    except FileNotFoundError:
        console.print(f"[bold red]Error: File '{chat_json_path}' not found.[/]")
        return

    # Load the embedder
    embedder = archive.load_embedder()

    conversations = data if isinstance(data, list) else data.get("conversations", [])
    console.print(
        f"[bold]Found {len(conversations)} conversations in the JSON file.[/]"
    )

    documents, metadatas, ids = [], [], []

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing conversations..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(conversations))

        for idx, conv in enumerate(conversations):
            progress.update(task, advance=1)

            messages = []
            message_map = conv.get("mapping", {})
            ordered = sorted(
                message_map.values(), key=lambda m: m.get("create_time", 0)
            )

            # Store all timestamps for more comprehensive filtering
            timestamps = []

            for msg in ordered:
                message = msg.get("message")
                if not message:
                    continue

                role = message.get("author", {}).get("role", "unknown")
                parts = message.get("content", {}).get("parts", [])
                content = " ".join([p for p in parts if isinstance(p, str)]).strip()
                if not content:
                    continue

                ts = message.get("create_time", None)
                if ts:
                    timestamps.append(ts)
                dt_str = format_timestamp(ts)

                if role == "user":
                    messages.append(f"**You said** *(on {dt_str})*:\n\n{content}")
                elif role == "assistant":
                    messages.append(f"**ChatGPT said** *(on {dt_str})*:\n\n{content}")
                else:
                    messages.append(
                        f"*{role.capitalize()}* *(on {dt_str})*:\n\n{content}"
                    )

            # Calculate earliest and latest timestamps
            valid_timestamps = [ts for ts in timestamps if ts]
            earliest_ts = min(valid_timestamps) if valid_timestamps else None
            latest_ts = max(valid_timestamps) if valid_timestamps else None

            # Convert timestamps to ISO format for better filtering
            earliest_ts_iso = (
                datetime.fromtimestamp(earliest_ts).isoformat() if earliest_ts else None
            )
            latest_ts_iso = (
                datetime.fromtimestamp(latest_ts).isoformat() if latest_ts else None
            )

            full_text = "\n\n".join(messages)

            # Skip empty conversations
            if not full_text.strip():
                continue

            # Handle chunking if enabled
            if chunk_size > 0 and len(messages) > chunk_size:
                chunks = [
                    messages[i : i + chunk_size]
                    for i in range(0, len(messages), chunk_size)
                ]

                for chunk_idx, chunk in enumerate(chunks):
                    chunk_text = "\n\n".join(chunk)
                    chunk_title = f"{conv.get('title', f'Chat {idx}')} (Part {chunk_idx+1}/{len(chunks)})"

                    documents.append(chunk_text)

                    # Create a metadata dict with no None values
                    metadata_dict = {
                        "title": chunk_title,
                        "source": "json",
                        "message_count": len(chunk),
                        "is_chunk": True,
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks),
                    }

                    # Only add fields that aren't None
                    if conv.get("id"):
                        metadata_dict["id"] = f"{conv.get('id')}-{chunk_idx}"
                        metadata_dict["conversation_id"] = conv.get("id")
                    if earliest_ts_iso:
                        metadata_dict["earliest_ts"] = earliest_ts_iso
                    if latest_ts_iso:
                        metadata_dict["latest_ts"] = latest_ts_iso

                    metadatas.append(metadata_dict)
                    ids.append(f"chat-{idx}-chunk-{chunk_idx}")
            else:
                documents.append(full_text)

                # Create a metadata dict with no None values
                metadata_dict = {
                    "title": conv.get("title", f"Chat {idx}"),
                    "source": "json",
                    "message_count": len(messages),
                    "is_chunk": False,
                }

                # Only add fields that aren't None
                if conv.get("id"):
                    metadata_dict["id"] = conv.get("id")
                if earliest_ts_iso:
                    metadata_dict["earliest_ts"] = earliest_ts_iso
                if latest_ts_iso:
                    metadata_dict["latest_ts"] = latest_ts_iso

                metadatas.append(metadata_dict)
                ids.append(f"chat-{idx}")

    if not documents:
        console.print("[bold red]No valid conversations found to index.[/]")
        return

    console.print(f"[bold]Embedding {len(documents)} documents...[/]")

    # Process in batches to avoid memory issues with large datasets
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        end = min(i + batch_size, len(documents))

        with console.status(
            f"[bold green]Embedding batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}..."
        ):
            batch_docs = documents[i:end]
            batch_metas = metadatas[i:end]
            batch_ids = ids[i:end]

            embeddings = embedder.encode(batch_docs, show_progress_bar=False)

            archive.add_documents(
                documents=batch_docs,
                embeddings=embeddings.tolist(),
                metadatas=batch_metas,
                ids=batch_ids,
            )

    console.print("[bold green]‚úÖ Chat indexing complete![/]")
    console.print(f"[bold]Total documents indexed: {len(documents)}[/]")


if __name__ == "__main__":
    import sys
    try:
        app()
    except Exception as e:
        console.print(f"[bold red]Error: {str(e)}[/]")
        sys.exit(1)
