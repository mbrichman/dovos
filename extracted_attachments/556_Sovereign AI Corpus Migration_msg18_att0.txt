import json
import os
import typer
from typing import Optional, List, Tuple
from docx import Document
from sentence_transformers import SentenceTransformer
import chromadb
import re
from datetime import datetime, timedelta
import tempfile
import markdown
import logging
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Rich console for better output
console = Console()

app = typer.Typer(help="Semantic search tool for ChatGPT conversations")

# === CONFIG ===
COLLECTION_NAME = "chat_history"
PERSIST_DIR = "./chroma_storage"
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"


# Create a class to handle all database operations
class ChatArchive:
    def __init__(self):
        self.embedder = None
        self.chroma_client = None
        self.collection = None
        self._initialize()

    def _initialize(self):
        """Initialize embedding model and database connection"""
        # Initialize chromadb client
        self.chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)

        # Initialize collection
        self.collection = self.chroma_client.get_or_create_collection(
            COLLECTION_NAME,
            metadata={
                "description": "ChatGPT conversation history with timestamp indexing"
            },
        )

    def load_embedder(self):
        """Load the embedding model (done lazily to save resources)"""
        if self.embedder is None:
            with console.status("[bold green]Loading embedding model..."):
                self.embedder = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)
        return self.embedder

    def get_count(self):
        """Get the number of documents in the collection"""
        return self.collection.count()

    def delete_collection(self):
        """Delete the collection"""
        self.chroma_client.delete_collection(COLLECTION_NAME)
        # Reinitialize
        self.collection = self.chroma_client.create_collection(COLLECTION_NAME)

    def add_documents(self, documents, embeddings, metadatas, ids):
        """Add documents to the collection"""
        self.collection.add(
            documents=documents, embeddings=embeddings, metadatas=metadatas, ids=ids
        )

    def query(self, query_embeddings, n_results=5, where=None):
        """Query the collection"""
        if where:
            return self.collection.query(
                query_embeddings=query_embeddings, n_results=n_results, where=where
            )
        else:
            return self.collection.query(
                query_embeddings=query_embeddings, n_results=n_results
            )

    def get_documents(self, where=None, include=None, limit=None):
        """Get documents from the collection"""
        kwargs = {}
        if where:
            kwargs["where"] = where
        if include:
            kwargs["include"] = include
        if limit:
            kwargs["limit"] = limit

        return self.collection.get(**kwargs)


# Create global archive instance
archive = ChatArchive()


def format_timestamp(ts):
    """Format a Unix timestamp into a human-readable string"""
    if not ts:
        return "unknown time"
    try:
        return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")
    except (TypeError, ValueError):
        return "invalid time"


def parse_date_range(date_range: str):
    """Parse a date range string into start and end timestamps"""
    now = datetime.now()

    if date_range.lower() == "last week":
        start_date = now - timedelta(days=7)
        end_date = now
    elif date_range.lower() == "last month":
        start_date = now - timedelta(days=30)
        end_date = now
    elif date_range.lower() == "last year":
        start_date = now - timedelta(days=365)
        end_date = now
    elif " to " in date_range:
        start_str, end_str = date_range.split(" to ")
        try:
            start_date = datetime.fromisoformat(start_str)
            end_date = datetime.fromisoformat(end_str)
            # Set end date to end of day
            end_date = end_date.replace(hour=23, minute=59, second=59)
        except ValueError:
            console.print("[bold red]Invalid date format. Use YYYY-MM-DD format.[/]")
            return None, None
    else:
        # Single date
        try:
            start_date = datetime.fromisoformat(date_range)
            end_date = start_date.replace(hour=23, minute=59, second=59)
        except ValueError:
            console.print("[bold red]Invalid date format. Use YYYY-MM-DD format.[/]")
            return None, None

    return start_date.isoformat(), end_date.isoformat()


@app.command()
def search(
    query: str = typer.Argument(..., help="Search query"),
    top_k: int = typer.Option(5, "--top", "-t", help="Number of results to return"),
    keyword: bool = typer.Option(
        False,
        "--keyword",
        "-k",
        help="Use literal keyword search instead of semantic search",
    ),
    date_range: Optional[str] = typer.Option(
        None,
        "--date",
        "-d",
        help="Filter by date range (e.g., '2023-01-01 to 2023-01-31' or 'last week')",
    ),
    export: bool = typer.Option(
        False,
        "--export",
        "-e",
        help="Export results to markdown files",
    ),
):
    """Search through indexed conversations semantically or by keywords"""
    console.print(f"[bold]üîç Running search...[/]")
    doc_count = archive.get_count()
    console.print(f"[bold]üìÑ Document count in DB:[/] {doc_count}")

    if doc_count == 0:
        console.print(
            "[bold red]No documents found in the database. Run the 'index' command first.[/]"
        )
        raise typer.Exit(code=1)

    # Handle date filtering
    date_filter = None
    if date_range:
        start_date, end_date = parse_date_range(date_range)
        if start_date and end_date:
            console.print(
                f"[bold]üìÖ Filtering by date range:[/] {start_date} to {end_date}"
            )
            date_filter = {
                "$and": [
                    {"earliest_ts": {"$gte": start_date}},
                    {"latest_ts": {"$lte": end_date}},
                ]
            }

    results = {"documents": [[]], "metadatas": [[]]}

    if keyword:
        console.print("[bold]üîé Using keyword-based search.[/]")
        # Get all docs matching date filter if specified
        where_filter = date_filter if date_filter else None
        all_docs = archive.get_documents(
            where=where_filter, include=["documents", "metadatas"], limit=9999
        )
        terms = (
            query.replace("AND", "&&").replace("OR", "||").replace("NOT", "!!").split()
        )
        matches = []

        def match(doc):
            text = doc.lower()
            expr = ""
            for term in terms:
                if term == "&&":
                    expr += " and "
                elif term == "||":
                    expr += " or "
                elif term == "!!":
                    expr += " not "
                else:
                    expr += f"'{term.lower()}' in text"
            try:
                return eval(expr)
            except Exception:
                return False

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold green]Processing keyword search..."),
            transient=True,
        ) as progress:
            task = progress.add_task("Searching", total=len(all_docs["documents"]))

            for idx, doc in enumerate(all_docs["documents"]):
                if match(doc):
                    matches.append((doc, all_docs["metadatas"][idx]))
                progress.update(task, advance=1)

        if not matches:
            console.print("[bold red]‚ùå No keyword matches found.[/]")
            return

        console.print(f"[bold green]‚úÖ Found {len(matches)} keyword matches.[/]")
        for idx, (doc, meta) in enumerate(matches[:top_k]):
            console.print(
                f"\n[bold cyan]Result {idx+1}:[/] {meta.get('title', 'Untitled')}"
            )
            console.print(f"[dim]Date: {meta.get('earliest_ts', 'Unknown')}[/]")
            console.print(doc[:300] + "...\n")
            results["documents"][0].append(doc)
            results["metadatas"][0].append(meta)

    else:
        # Vector search
        # Load embedder for semantic search
        embedder = archive.load_embedder()

        with console.status("[bold green]Running semantic search..."):
            embedding = embedder.encode([query])[0]

            # Perform the query with date filter if specified
            results = archive.query(
                query_embeddings=[embedding.tolist()],
                n_results=top_k,
                where=date_filter,
            )

        if not results["documents"][0]:
            console.print("[bold red]‚ùå No semantic matches found.[/]")
            return

        for idx, (doc, meta) in enumerate(
            zip(results["documents"][0], results["metadatas"][0])
        ):
            console.print(
                f"\n[bold cyan]Result {idx+1}:[/] {meta.get('title', 'Untitled')}"
            )
            console.print(f"[dim]Date: {meta.get('earliest_ts', 'Unknown')}[/]")
            console.print(doc[:300] + "...\n")

    # Export results if requested
    if export:
        export_dir = "search_results"
        os.makedirs(export_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for idx, (doc, meta) in enumerate(
            zip(results["documents"][0], results["metadatas"][0])
        ):
            title = meta.get("title", f"Result_{idx}").replace(" ", "_")[:30]
            filename = f"{export_dir}/{timestamp}_{title}.md"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"# {meta.get('title', 'Untitled')}\n\n")
                f.write(f"Date: {meta.get('earliest_ts', 'Unknown')}\n\n")
                f.write(doc)

            console.print(f"[bold green]Saved result {idx+1} to {filename}[/]")

        raise typer.Exit()

    # Interactive result viewing
    try:
        choice = console.input(
            "\n[bold]Enter result number to view as HTML in Safari (or press Enter to skip):[/] "
        ).strip()
    except (EOFError, KeyboardInterrupt):
        console.print("\n[dim]‚è≠Ô∏è Skipped.[/]")
        return

    if not choice:
        return

    if choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(results["documents"][0]):
            full_text = results["documents"][0][idx]
            meta = results["metadatas"][0][idx]
            title = meta.get("title", "Untitled").replace(" ", "_")[:30]
            date_str = meta.get("earliest_ts", "Unknown")

            with open("debug_output.md", "w", encoding="utf-8") as f:
                f.write(full_text)

            html_body = markdown.markdown(
                full_text, extensions=["sane_lists", "tables", "nl2br"]
            )
            html_full = f"""
            <html><head><meta charset="UTF-8"><title>{title}</title>
            <style>
            body {{ font-family: Georgia, serif; max-width: 700px; margin: auto; line-height: 1.6; padding: 20px; }}
            h1 {{ color: #2c3e50; }}
            .metadata {{ color: #7f8c8d; margin-bottom: 20px; }}
            strong {{ color: #2980b9; }}
            pre {{ background-color: #f8f9fa; padding: 10px; border-radius: 5px; overflow-x: auto; }}
            code {{ font-family: 'Courier New', monospace; }}
            </style></head>
            <body>
            <h1>{meta.get('title', 'Untitled')}</h1>
            <div class="metadata">Date: {date_str}</div>
            {html_body}
            </body></html>"""

            with tempfile.NamedTemporaryFile(
                "w", delete=False, suffix=".html", encoding="utf-8"
            ) as tmp:
                tmp.write(html_full)
                tmp_path = tmp.name

            console.print(f"[bold green]Opening result in Safari...[/]")
            os.system(f'open -a Safari "{tmp_path}"')
        else:
            console.print("[bold red]‚ùå Invalid result number.[/]")
    else:
        console.print("[bold red]‚ùå Invalid input.[/]")


@app.command()
def index(
    chat_json_path: str = typer.Argument(
        ..., help="Path to your conversations.json file"
    ),
    chunk_size: int = typer.Option(
        0,
        "--chunk",
        "-c",
        help="Split conversations into chunks of this size (0 for no chunking)",
    ),
    overwrite: bool = typer.Option(
        False,
        "--overwrite",
        "-o",
        help="Overwrite existing collection",
    ),
):
    """Index ChatGPT conversations from a JSON export"""
    # Check if collection already exists and has documents
    doc_count = archive.get_count()
    if doc_count > 0 and not overwrite:
        console.print(
            f"[bold yellow]Collection already contains {doc_count} documents.[/]"
        )
        if not typer.confirm("Do you want to add to the existing collection?"):
            if typer.confirm(
                "Do you want to delete the existing collection and start fresh?"
            ):
                console.print("[bold red]Deleting existing collection...[/]")
                archive.delete_collection()
            else:
                console.print("[bold]Operation cancelled.[/]")
                return

    try:
        with open(chat_json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError:
        console.print("[bold red]Error: Invalid JSON file.[/]")
        return
    except FileNotFoundError:
        console.print(f"[bold red]Error: File '{chat_json_path}' not found.[/]")
        return

    # Load the embedder
    embedder = archive.load_embedder()

    conversations = data if isinstance(data, list) else data.get("conversations", [])
    console.print(
        f"[bold]Found {len(conversations)} conversations in the JSON file.[/]"
    )

    documents, metadatas, ids = [], [], []

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing conversations..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(conversations))

        for idx, conv in enumerate(conversations):
            progress.update(task, advance=1)

            messages = []
            message_map = conv.get("mapping", {})
            ordered = sorted(
                message_map.values(), key=lambda m: m.get("create_time", 0)
            )

            # Store all timestamps for more comprehensive filtering
            timestamps = []

            for msg in ordered:
                message = msg.get("message")
                if not message:
                    continue

                role = message.get("author", {}).get("role", "unknown")
                parts = message.get("content", {}).get("parts", [])
                content = " ".join([p for p in parts if isinstance(p, str)]).strip()
                if not content:
                    continue

                ts = message.get("create_time", None)
                if ts:
                    timestamps.append(ts)
                dt_str = format_timestamp(ts)

                if role == "user":
                    messages.append(f"**You said** *(on {dt_str})*:\n\n{content}")
                elif role == "assistant":
                    messages.append(f"**ChatGPT said** *(on {dt_str})*:\n\n{content}")
                else:
                    messages.append(
                        f"*{role.capitalize()}* *(on {dt_str})*:\n\n{content}"
                    )

            # Calculate earliest and latest timestamps
            valid_timestamps = [ts for ts in timestamps if ts]
            earliest_ts = min(valid_timestamps) if valid_timestamps else None
            latest_ts = max(valid_timestamps) if valid_timestamps else None

            # Convert timestamps to ISO format for better filtering
            earliest_ts_iso = (
                datetime.fromtimestamp(earliest_ts).isoformat() if earliest_ts else None
            )
            latest_ts_iso = (
                datetime.fromtimestamp(latest_ts).isoformat() if latest_ts else None
            )

            full_text = "\n\n".join(messages)

            # Skip empty conversations
            if not full_text.strip():
                continue

            # Handle chunking if enabled
            if chunk_size > 0 and len(messages) > chunk_size:
                chunks = [
                    messages[i : i + chunk_size]
                    for i in range(0, len(messages), chunk_size)
                ]

                for chunk_idx, chunk in enumerate(chunks):
                    chunk_text = "\n\n".join(chunk)
                    chunk_title = f"{conv.get('title', f'Chat {idx}')} (Part {chunk_idx+1}/{len(chunks)})"

                    documents.append(chunk_text)

                    # Create a metadata dict with no None values
                    metadata_dict = {
                        "title": chunk_title,
                        "source": "json",
                        "message_count": len(chunk),
                        "is_chunk": True,
                        "chunk_index": chunk_idx,
                        "total_chunks": len(chunks),
                    }

                    # Only add fields that aren't None
                    if conv.get("id"):
                        metadata_dict["id"] = f"{conv.get('id')}-{chunk_idx}"
                        metadata_dict["conversation_id"] = conv.get("id")
                    if earliest_ts_iso:
                        metadata_dict["earliest_ts"] = earliest_ts_iso
                    if latest_ts_iso:
                        metadata_dict["latest_ts"] = latest_ts_iso

                    metadatas.append(metadata_dict)
                    ids.append(f"chat-{idx}-chunk-{chunk_idx}")
            else:
                documents.append(full_text)

                # Create a metadata dict with no None values
                metadata_dict = {
                    "title": conv.get("title", f"Chat {idx}"),
                    "source": "json",
                    "message_count": len(messages),
                    "is_chunk": False,
                }

                # Only add fields that aren't None
                if conv.get("id"):
                    metadata_dict["id"] = conv.get("id")
                if earliest_ts_iso:
                    metadata_dict["earliest_ts"] = earliest_ts_iso
                if latest_ts_iso:
                    metadata_dict["latest_ts"] = latest_ts_iso

                metadatas.append(metadata_dict)
                ids.append(f"chat-{idx}")

    if not documents:
        console.print("[bold red]No valid conversations found to index.[/]")
        return

    console.print(f"[bold]Embedding {len(documents)} documents...[/]")

    # Process in batches to avoid memory issues with large datasets
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        end = min(i + batch_size, len(documents))

        with console.status(
            f"[bold green]Embedding batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}..."
        ):
            batch_docs = documents[i:end]
            batch_metas = metadatas[i:end]
            batch_ids = ids[i:end]

            embeddings = embedder.encode(batch_docs, show_progress_bar=False)

            archive.add_documents(
                documents=batch_docs,
                embeddings=embeddings.tolist(),
                metadatas=batch_metas,
                ids=batch_ids,
            )

    console.print("[bold green]‚úÖ Chat indexing complete![/]")
    console.print(f"[bold]Total documents indexed: {len(documents)}[/]")


@app.command()
def index_docs(
    doc_folder: str = typer.Argument(..., help="Folder containing .docx files"),
    overwrite: bool = typer.Option(
        False,
        "--overwrite",
        "-o",
        help="Overwrite existing documents from the same source",
    ),
):
    """Index Word documents containing chat conversations"""
    if overwrite:
        # Delete existing documents from the 'docx' source
        existing_docs = archive.get_documents(
            where={"source": "docx"}, include=["metadatas"]
        )
        if existing_docs["metadatas"]:
            existing_ids = [
                f"docx-{idx}" for idx in range(len(existing_docs["metadatas"]))
            ]
            archive.collection.delete(ids=existing_ids)
            console.print(
                f"[bold]Deleted {len(existing_ids)} existing Word documents.[/]"
            )

    # Load the embedder
    embedder = archive.load_embedder()

    documents, metadatas, ids = [], [], []

    def extract_timestamp(text):
        """Try to extract timestamps from text using regex"""
        # Look for common date formats
        date_patterns = [
            r"(\d{4}-\d{2}-\d{2})",  # YYYY-MM-DD
            r"(\d{2}/\d{2}/\d{4})",  # MM/DD/YYYY
            r"(\w+ \d{1,2}, \d{4})",  # Month DD, YYYY
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                try:
                    # Try different formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y"):
                        try:
                            dt = datetime.strptime(matches[0], fmt)
                            return dt.isoformat()
                        except ValueError:
                            continue
                except Exception:
                    pass
        return None

    def format_paragraphs(doc):
        lines = []
        timestamps = []
        current_role = None
        current_content = []

        for p in doc.paragraphs:
            text = p.text.strip()
            if not text:
                if current_role and current_content:
                    formatted_content = "\n".join(current_content)
                    lines.append(f"**{current_role}**:\n\n{formatted_content}\n")
                    current_content = []
                continue

            # Extract potential timestamp
            ts = extract_timestamp(text)
            if ts:
                timestamps.append(ts)

            # Check if this is a role indicator
            role_match = re.match(
                r"^(You|ChatGPT|User|Assistant|System)(\s+said)?:?$",
                text,
                re.IGNORECASE,
            )
            if role_match:
                # Save previous content if any
                if current_role and current_content:
                    formatted_content = "\n".join(current_content)
                    lines.append(f"**{current_role}**:\n\n{formatted_content}\n")

                current_role = role_match.group(1)
                current_content = []
            else:
                # Add to current content
                if current_role:
                    current_content.append(text)
                else:
                    lines.append(text)

        # Don't forget the last block
        if current_role and current_content:
            formatted_content = "\n".join(current_content)
            lines.append(f"**{current_role}**:\n\n{formatted_content}\n")

        return "\n".join(lines), timestamps

    # Check if folder exists
    if not os.path.isdir(doc_folder):
        console.print(f"[bold red]Error: Folder '{doc_folder}' not found.[/]")
        return

    docx_files = [
        f
        for f in os.listdir(doc_folder)
        if f.endswith(".docx")
        and not f.startswith("~$")
        and os.path.isfile(os.path.join(doc_folder, f))
    ]

    if not docx_files:
        console.print(f"[bold red]No .docx files found in '{doc_folder}'.[/]")
        return

    console.print(f"[bold]Found {len(docx_files)} Word documents.[/]")

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold green]Processing Word documents..."),
        transient=False,
    ) as progress:
        task = progress.add_task("Processing", total=len(docx_files))

        for idx, filename in enumerate(docx_files):
            progress.update(task, advance=1)

            full_path = os.path.join(doc_folder, filename)
            try:
                doc = Document(full_path)
                full_text, timestamps = format_paragraphs(doc)

                if not full_text.strip():
                    continue

                # Get file creation time as fallback timestamp
                file_create_time = datetime.fromtimestamp(
                    os.path.getctime(full_path)
                ).isoformat()

                # Use extracted timestamps if available, otherwise use file timestamps
                earliest_ts = min(timestamps) if timestamps else file_create_time
                latest_ts = max(timestamps) if timestamps else file_create_time

                documents.append(full_text)

                # Create metadata with no None values
                metadata_dict = {
                    "title": filename,
                    "source": "docx",
                    "file_path": full_path,
                }

                # Only add timestamps if they exist
                if earliest_ts:
                    metadata_dict["earliest_ts"] = earliest_ts
                if latest_ts:
                    metadata_dict["latest_ts"] = latest_ts

                metadatas.append(metadata_dict)
                ids.append(f"docx-{idx}")

            except Exception as e:
                console.print(f"[bold red]Error processing {filename}: {str(e)}[/]")

    if not documents:
        console.print("[bold red]No valid documents found to index.[/]")
        return

    console.print(f"[bold]Embedding {len(documents)} documents...[/]")

    # Process in batches
    batch_size = 100
    for i in range(0, len(documents), batch_size):
        end = min(i + batch_size, len(documents))

        with console.status(
            f"[bold green]Embedding batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}..."
        ):
            batch_docs = documents[i:end]
            batch_metas = metadatas[i:end]
            batch_ids = ids[i:end]

            embeddings = embedder.encode(batch_docs, show_progress_bar=False)

            archive.add_documents(
                documents=batch_docs,
                embeddings=embeddings.tolist(),
                metadatas=batch_metas,
                ids=batch_ids,
            )

    console.print("[bold green]‚úÖ Word document indexing complete![/]")
    console.print(f"[bold]Total documents indexed: {len(documents)}[/]")


@app.command()
def stats():
    """Show statistics about the indexed data"""
    doc_count = archive.get_count()
    if doc_count == 0:
        console.print(
            "[bold red]No documents found in the database. Run the 'index' command first.[/]"
        )
        return

    console.print(f"[bold]üìä Collection Statistics[/]")
    console.print(f"[bold]Total documents:[/] {doc_count}")

    # Get all metadata to analyze
    all_meta = archive.get_documents(include=["metadatas"], limit=9999)["metadatas"]

    # Count by source
    sources = {}
    for meta in all_meta:
        source = meta.get("source", "unknown")
        sources[source] = sources.get(source, 0) + 1

    console.print("\n[bold]Documents by source:[/]")
    for source, count in sources.items():
        console.print(f"  {source}: {count}")

    # Get date range
    dates = [meta.get("earliest_ts") for meta in all_meta if meta.get("earliest_ts")]
    if dates:
        dates.sort()
        console.print(f"\n[bold]Date range:[/] {dates[0]} to {dates[-1]}")

    # Count by chunks
    chunked = sum(1 for meta in all_meta if meta.get("is_chunk", False))
    console.print(f"\n[bold]Chunked documents:[/] {chunked}")
    console.print(f"[bold]Full documents:[/] {doc_count - chunked}")


@app.command()
def clear():
    """Clear all indexed data"""
    if typer.confirm("Are you sure you want to delete all indexed data?"):
        archive.delete_collection()
        console.print("[bold red]Collection deleted.[/]")
    else:
        console.print("[bold]Operation cancelled.[/]")


if __name__ == "__main__":
    app()
